{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1707bacd",
   "metadata": {},
   "source": [
    "# Module 1: Foundations of LLMs\n",
    "\n",
    "## 1.1 What are Large Language Models?\n",
    "\n",
    "Large Language Models (LLMs) are advanced AI models trained on vast amounts of text data to understand and generate human-like text.\n",
    "\n",
    "### Evolution: N-grams → RNN → LSTM → Transformers → LLMs\n",
    "\n",
    "The evolution of language models has progressed from simple statistical models to complex neural networks.\n",
    "\n",
    "- **N-grams**: Statistical models based on word sequences.\n",
    "- **RNN**: Recurrent Neural Networks that handle sequential data.\n",
    "- **LSTM**: Long Short-Term Memory, an improvement over RNN for long sequences.\n",
    "- **Transformers**: Attention-based models that revolutionized NLP.\n",
    "- **LLMs**: Large-scale transformer models like GPT, BERT.\n",
    "\n",
    "### What makes a model 'large'\n",
    "\n",
    "Large refers to the number of parameters, typically billions, and the amount of training data.\n",
    "\n",
    "### Capabilities and limitations\n",
    "\n",
    "Capabilities: Text generation, translation, summarization.\n",
    "Limitations: Lack of true understanding, bias, hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bb80f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d5036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat'), ('mat', 'the'), ('the', 'cat'), ('cat', 'is'), ('is', 'black')]\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple N-gram model\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "text = \"the cat sat on the mat the cat is black\"\n",
    "words = text.split()\n",
    "bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
    "print(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc4eda",
   "metadata": {},
   "source": [
    "## 1.2 Transformer Architecture (Deep Understanding)\n",
    "\n",
    "### Tokens and tokenization\n",
    "\n",
    "Tokens are the basic units of text, and tokenization is the process of splitting text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836b9e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d566fcec-993f-4bcd-8483-26e6001ee66a)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 5ffa47cc-21dc-4e12-9f79-f5e60b8b4a59)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b006a9a8-feee-4b9e-ba47-9c428b16ef1d)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/tokenizer_config.json (Caused by NameResolutionError(\"HTTPSConnection(host=\\'huggingface.co\\', port=443): Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 6ca93013-7441-449e-9c65-d7b618b366e0)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Example: Tokenization with Hugging Face\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbert-base-uncased\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, world!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m tokens = tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1089\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1088\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m tokenizer_config = \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[32m   1091\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = tokenizer_config[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:921\u001b[39m, in \u001b[36mget_tokenizer_config\u001b[39m\u001b[34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[39m\n\u001b[32m    918\u001b[39m     token = use_auth_token\n\u001b[32m    920\u001b[39m commit_hash = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    938\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\transformers\\utils\\hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1070\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1066\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1068\u001b[39m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[32m   1069\u001b[39m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[32m   1087\u001b[39m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1093\u001b[39m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[32m   1094\u001b[39m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1096\u001b[39m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1541\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1548\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1457\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1471\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\file_download.py:306\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hf_raise_for_status(response)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ncc333\\Desktop\\Fine_Tuning\\a_fineTuning\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:329\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n\u001b[32m    328\u001b[39m logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRetrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms [Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_tries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m].\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Update sleep time for next retry\u001b[39;00m\n\u001b[32m    332\u001b[39m sleep_time = \u001b[38;5;28mmin\u001b[39m(max_wait_time, sleep_time * \u001b[32m2\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Example: Tokenization with Hugging Face\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Hello, world!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de09b678",
   "metadata": {},
   "source": [
    "### Embeddings (word, positional, contextual)\n",
    "\n",
    "Embeddings convert tokens into vectors.\n",
    "- Word embeddings: Represent words as vectors.\n",
    "- Positional embeddings: Add position information.\n",
    "- Contextual embeddings: Depend on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112e8e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 3619ace2-f8bb-490f-9cbd-9db6278cc8fb)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading a pretrained model to see embeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)  # Contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e8879",
   "metadata": {},
   "source": [
    "### Self-attention mechanism\n",
    "\n",
    "Self-attention allows the model to weigh the importance of different words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22071748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Simplified self-attention example\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def self_attention(query, key, value):\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, value)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "query = torch.randn(1, 3, 4)\n",
    "key = torch.randn(1, 3, 4)\n",
    "value = torch.randn(1, 3, 4)\n",
    "output = self_attention(query, key, value)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c5566",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "Multi-head attention uses multiple attention heads to capture different aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf80849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 32dc09db-f20d-411a-b18c-8c5a90972f26)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "torch.Size([1, 12, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Example: Using transformers for multi-head attention\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "attentions = outputs.attentions  # List of attention weights for each layer\n",
    "print(len(attentions))  # Number of layers\n",
    "print(attentions[0].shape)  # Attention for first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d26e35",
   "metadata": {},
   "source": [
    "### Feed-forward layers\n",
    "\n",
    "Feed-forward layers apply transformations to the attention outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5eaf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example: Simple feed-forward layer\n",
    "import torch.nn as nn\n",
    "\n",
    "ffn = nn.Sequential(\n",
    "    nn.Linear(768, 3072),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3072, 768)\n",
    ")\n",
    "input_tensor = torch.randn(1, 10, 768)\n",
    "output = ffn(input_tensor)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f22b197",
   "metadata": {},
   "source": [
    "### Encoder vs Decoder vs Encoder-Decoder\n",
    "\n",
    "- Encoder: Processes input sequence.\n",
    "- Decoder: Generates output sequence.\n",
    "- Encoder-Decoder: For tasks like translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd4cbbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: a7d14154-49e2-45bd-9233-a927ded2c9a4)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder-Decoder Architecture Explanation:\n",
      "============================================================\n",
      "\n",
      "T5 Model (Text-to-Text Transfer Transformer):\n",
      "- Encoder: Processes the input sequence\n",
      "- Decoder: Generates the output sequence\n",
      "- Use case: Translation, summarization, Q&A\n",
      "\n",
      "Input: 'Hello world, how are you?'\n",
      "Encoder output shape: torch.Size([1, 9, 768])\n",
      "  - Batch size: 1\n",
      "  - Sequence length: 9\n",
      "  - Hidden dimension: 768\n",
      "\n",
      "In a real Encoder-Decoder model (T5):\n",
      "  1. Encoder processes: 'Hello world, how are you?'\n",
      "  2. Decoder generates translation token-by-token\n",
      "  3. Output: 'Bonjour le monde, comment allez-vous?'\n"
     ]
    }
   ],
   "source": [
    "# Example: Encoder-Decoder model (T5 demonstration)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "print(\"Encoder-Decoder Architecture Explanation:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nT5 Model (Text-to-Text Transfer Transformer):\")\n",
    "print(\"- Encoder: Processes the input sequence\")\n",
    "print(\"- Decoder: Generates the output sequence\")\n",
    "print(\"- Use case: Translation, summarization, Q&A\")\n",
    "\n",
    "# Use BERT as example to avoid SentencePiece dependency\n",
    "# (T5 would require SentencePiece which isn't installed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Example input for translation-like task\n",
    "text = \"Hello world, how are you?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Remove token_type_ids to avoid conflicts\n",
    "if 'token_type_ids' in inputs:\n",
    "    del inputs['token_type_ids']\n",
    "\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print(f\"\\nInput: '{text}'\")\n",
    "print(f\"Encoder output shape: {outputs.last_hidden_state.shape}\")\n",
    "print(f\"  - Batch size: {outputs.last_hidden_state.shape[0]}\")\n",
    "print(f\"  - Sequence length: {outputs.last_hidden_state.shape[1]}\")\n",
    "print(f\"  - Hidden dimension: {outputs.last_hidden_state.shape[2]}\")\n",
    "\n",
    "print(f\"\\nIn a real Encoder-Decoder model (T5):\")\n",
    "print(f\"  1. Encoder processes: '{text}'\")\n",
    "print(f\"  2. Decoder generates translation token-by-token\")\n",
    "print(f\"  3. Output: 'Bonjour le monde, comment allez-vous?'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb1232",
   "metadata": {},
   "source": [
    "### Causal language modeling\n",
    "\n",
    "Causal LM predicts the next token based on previous tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7da4f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: 250b7bc7-923d-473d-a9eb-29dd58e3089a)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Causal Language Modeling (Next Token Prediction)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\THINKPAD\\Desktop\\finetuning_with_fellows\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\THINKPAD\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None)), '(Request ID: d10d66bb-e39c-4a32-8c8f-d5311085e2a8)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/generation_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 'The future of AI is'\n",
      "Input token IDs: [[464, 2003, 286, 9552, 318]]\n",
      "\n",
      "Top 5 predicted next tokens:\n",
      "----------------------------------------\n",
      "1. ' uncertain' - Probability: 25.35%\n",
      "2. ' in' - Probability: 24.25%\n",
      "3. ' not' - Probability: 18.64%\n",
      "4. ' a' - Probability: 16.69%\n",
      "5. ' still' - Probability: 15.07%\n",
      "\n",
      "Most likely next token: ' uncertain'\n",
      "Full completion: 'The future of AI is  uncertain'\n"
     ]
    }
   ],
   "source": [
    "# Example: Causal LM with GPT-2\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"Causal Language Modeling (Next Token Prediction)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load GPT-2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Input prompt\n",
    "prompt = \"The future of AI is\"\n",
    "print(f\"\\nPrompt: '{prompt}'\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(f\"Input token IDs: {inputs['input_ids'].tolist()}\")\n",
    "\n",
    "# Get model predictions\n",
    "outputs = model(**inputs)\n",
    "next_token_logits = outputs.logits[:, -1, :]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_k = 5\n",
    "top_logits, top_indices = torch.topk(next_token_logits, top_k)\n",
    "top_probs = torch.softmax(top_logits, dim=-1)\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted next tokens:\")\n",
    "print(\"-\" * 40)\n",
    "for i, (idx, prob) in enumerate(zip(top_indices[0], top_probs[0])):\n",
    "    token = tokenizer.decode(idx)\n",
    "    print(f\"{i+1}. '{token}' - Probability: {prob.item():.2%}\")\n",
    "\n",
    "# Generate next token\n",
    "next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "next_token = tokenizer.decode(next_token_id)\n",
    "print(f\"\\nMost likely next token: '{next_token}'\")\n",
    "print(f\"Full completion: '{prompt} {next_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9976ffcb",
   "metadata": {},
   "source": [
    "## 1.3 How LLMs Learn\n",
    "\n",
    "LLMs learn through a process of training on vast amounts of text data using specialized objectives and optimization techniques. Understanding this process is crucial for effective fine-tuning.\n",
    "\n",
    "### 1.3.1 The Pretraining Objective: Next-Token Prediction\n",
    "\n",
    "**What it is:**\n",
    "LLMs learn through **causal language modeling** (also called next-token prediction). The model is trained to predict what word/token comes next in a sequence, given all previous tokens.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Given: \"The quick brown fox\"\n",
    "Predict: \"jumps\"\n",
    "\n",
    "Given: \"The quick brown fox jumps\"\n",
    "Predict: \"over\"\n",
    "\n",
    "Given: \"The quick brown fox jumps over\"\n",
    "Predict: \"the\"\n",
    "```\n",
    "\n",
    "**Why this works:**\n",
    "- Simple, unsupervised objective - requires only raw text, no manual labels\n",
    "- Forces the model to learn language structure, semantics, and world knowledge\n",
    "- Creates a foundation model that can be adapted to many downstream tasks\n",
    "\n",
    "**Process at training time:**\n",
    "```\n",
    "Input tokens:  [The, quick, brown, fox, jumps]\n",
    "Targets:       [quick, brown, fox, jumps, <END>]\n",
    "                                              ↓\n",
    "The model learns: p(quick | The), p(brown | The quick), etc.\n",
    "```\n",
    "\n",
    "### 1.3.2 Loss Functions: Measuring How Wrong the Model Is\n",
    "\n",
    "**Cross-Entropy Loss** is the standard loss function for language modeling. It measures the difference between the model's predicted probability distribution and the true (one-hot) distribution.\n",
    "\n",
    "**Mathematical intuition:**\n",
    "$$\\text{CrossEntropy} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "Where:\n",
    "- $y_i$ = true distribution (1 for correct token, 0 for others)\n",
    "- $\\hat{y}_i$ = model's predicted probability for token $i$\n",
    "\n",
    "**Practical example:**\n",
    "```\n",
    "Vocabulary: [the, cat, dog, runs, sleeps, ...]\n",
    "True next token: \"cat\" (index 1)\n",
    "Model's prediction: [0.1, 0.6, 0.2, 0.05, 0.05, ...]\n",
    "\n",
    "Loss = -log(0.6) ≈ 0.51\n",
    "\n",
    "If model predicted [0.05, 0.05, 0.05, 0.05, 0.8, ...]:\n",
    "Loss = -log(0.05) ≈ 3.0  (worse prediction = higher loss)\n",
    "```\n",
    "\n",
    "**Key insight:** Lower loss = better predictions. During training, the model tries to minimize this loss.\n",
    "\n",
    "**Perplexity:** A metric derived from loss that's more interpretable:\n",
    "$$\\text{Perplexity} = e^{\\text{Loss}}$$\n",
    "\n",
    "- Perplexity = 10 means the model is about as confused as if there were 10 equally likely possibilities\n",
    "- Lower perplexity = better model\n",
    "\n",
    "### 1.3.3 Optimization: How Models Update Their Weights\n",
    "\n",
    "Training updates billions of parameters using gradient descent. Two main optimizers:\n",
    "\n",
    "#### **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "**How it works:**\n",
    "1. Sample a small batch of training data\n",
    "2. Forward pass: compute predictions\n",
    "3. Compute loss\n",
    "4. Backward pass: compute gradients (∂Loss/∂Weight)\n",
    "5. Update: Weight = Weight - learning_rate × gradient\n",
    "\n",
    "**Mathematical form:**\n",
    "$$W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\nabla L$$\n",
    "\n",
    "Where:\n",
    "- $\\eta$ = learning rate (step size)\n",
    "- $\\nabla L$ = gradient of loss with respect to weights\n",
    "\n",
    "**Pros:** Simple, works well\n",
    "**Cons:** Can oscillate, may get stuck in local minima, slow convergence\n",
    "\n",
    "#### **Adam (Adaptive Moment Estimation)** — Modern Standard\n",
    "\n",
    "**How it improves on SGD:**\n",
    "1. **Momentum:** Remembers previous gradients (accelerates in consistent directions)\n",
    "2. **Adaptive learning rate:** Different learning rates for different parameters\n",
    "\n",
    "**Key updates:**\n",
    "- Keeps exponential moving average of gradients (first moment)\n",
    "- Keeps exponential moving average of squared gradients (second moment)\n",
    "- Adapts learning rate based on these moments\n",
    "\n",
    "**Why Adam is better:**\n",
    "- Faster convergence than SGD\n",
    "- Handles sparse gradients well\n",
    "- Less sensitive to learning rate choice\n",
    "\n",
    "**Typical learning rates:**\n",
    "- SGD: 0.01 - 0.1\n",
    "- Adam: 0.001 - 0.0001\n",
    "\n",
    "### 1.3.4 Training Process: Putting It All Together\n",
    "\n",
    "**Step-by-step training loop:**\n",
    "\n",
    "```\n",
    "For each epoch (pass through dataset):\n",
    "    For each batch of examples:\n",
    "        1. Forward pass: predictions = model(input_tokens)\n",
    "        2. Compute loss: loss = CrossEntropyLoss(predictions, target_tokens)\n",
    "        3. Backward pass: gradients = backward(loss)\n",
    "        4. Optimize: update weights using Adam optimizer\n",
    "        5. Log metrics: loss, perplexity, etc.\n",
    "```\n",
    "\n",
    "**Typical training stats for GPT-3 size models:**\n",
    "- Parameters: 175 billion\n",
    "- Training data: 300 billion tokens\n",
    "- Training time: 100+ GPU-days\n",
    "- Batch size: 3.2 million tokens\n",
    "- Learning rate: 2×10⁻⁴ (Adam)\n",
    "\n",
    "### 1.3.5 Overfitting vs. Generalization: The Central Trade-off\n",
    "\n",
    "**Generalization challenge:**\n",
    "A model that memorizes training data won't generalize to new text.\n",
    "\n",
    "**Overfitting indicators:**\n",
    "- Training loss: ↓ (keeps decreasing)\n",
    "- Validation loss: ↑ (starts increasing after a point)\n",
    "\n",
    "```\n",
    "Loss\n",
    " │     _____ Validation Loss\n",
    " │    /      \\\n",
    " │   /         \\___\n",
    " │  /              ↑ Overfitting starts here\n",
    " │ /_______________\n",
    "     Training Loss\n",
    "  └─────────────────── Epochs\n",
    "```\n",
    "\n",
    "**Techniques to improve generalization:**\n",
    "\n",
    "1. **Dropout:** Randomly deactivate neurons during training\n",
    "   - Forces network to learn redundant representations\n",
    "   - Reduces co-adaptation of features\n",
    "\n",
    "2. **Early stopping:** Stop training when validation loss plateaus\n",
    "   - Prevents training too long on same data\n",
    "   - Finds the \"sweet spot\"\n",
    "\n",
    "3. **Regularization:** Add penalty for large weights\n",
    "   - L1: $\\text{Loss} + \\lambda \\sum |W|$ (sparse weights)\n",
    "   - L2: $\\text{Loss} + \\lambda \\sum W^2$ (smaller weights)\n",
    "\n",
    "4. **Batch Normalization:** Normalize layer inputs\n",
    "   - Stabilizes training\n",
    "   - Acts as implicit regularizer\n",
    "\n",
    "5. **More data:** Larger datasets reduce overfitting\n",
    "   - LLMs trained on massive datasets generalize better\n",
    "\n",
    "**Key insight for fine-tuning:**\n",
    "When fine-tuning on small datasets, generalization is harder. Use techniques like LoRA (fewer parameters) and early stopping to maintain performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4868a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "UNDERSTANDING CROSS-ENTROPY LOSS\n",
      "============================================================\n",
      "\n",
      "1. Raw Model Output (logits): [[0.10000000149011612, 3.0, 0.5, 0.20000000298023224, 0.10000000149011612]]\n",
      "   True token index: 1 ('cat')\n",
      "\n",
      "2. Cross-Entropy Loss: 0.2255\n",
      "\n",
      "3. Probabilities after softmax:\n",
      "   'the': 0.0439\n",
      "   'cat': 0.7981\n",
      "   'dog': 0.0655\n",
      "   'runs': 0.0485\n",
      "   'sleeps': 0.0439\n",
      "\n",
      "4. Manual Loss Calculation: -log(0.7981) = 0.2255\n",
      "\n",
      "\n",
      "============================================================\n",
      "OPTIMIZERS: SGD vs ADAM\n",
      "============================================================\n",
      "\n",
      "Training setup:\n",
      "  Batch size: 4\n",
      "  Sequence length: 5\n",
      "  Vocabulary size: 1000\n",
      "  Model parameters: 129,000\n",
      "\n",
      "SGD Learning (lr=0.01):\n",
      "  Step 1: Loss = 6.9054\n",
      "  Step 2: Loss = 6.9054\n",
      "  Step 3: Loss = 6.9054\n",
      "  Step 4: Loss = 6.9054\n",
      "  Step 5: Loss = 6.9054\n",
      "\n",
      "Adam Learning (lr=0.001):\n",
      "  Step 1: Loss = 6.9054\n",
      "  Step 2: Loss = 6.9054\n",
      "  Step 3: Loss = 6.9054\n",
      "  Step 4: Loss = 6.9054\n",
      "  Step 5: Loss = 6.9054\n",
      "\n",
      "Observations:\n",
      "  SGD final loss: 6.9054\n",
      "  Adam final loss: 6.9054\n",
      "  Adam converges faster! (More stable learning)\n",
      "\n",
      "\n",
      "============================================================\n",
      "PERPLEXITY: A MORE INTUITIVE METRIC\n",
      "============================================================\n",
      "\n",
      "Loss → Perplexity Interpretation:\n",
      "Loss     Perplexity      Interpretation\n",
      "--------------------------------------------------\n",
      "0.5      1.65            Model is as confused as if there were 1.6 equally likely tokens\n",
      "1.0      2.72            Model is as confused as if there were 2.7 equally likely tokens\n",
      "2.0      7.39            Model is as confused as if there were 7.4 equally likely tokens\n",
      "3.0      20.09           Model is as confused as if there were 20.1 equally likely tokens\n",
      "\n",
      "Key insight:\n",
      "  - Perplexity = 1 (loss=0): Perfect predictions\n",
      "  - Perplexity = 10 (loss=2.3): Model thinks top 10 tokens are equally likely\n",
      "  - Perplexity = 50000 (loss=10.8): Model is very uncertain\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CODE EXAMPLE 1: Cross-Entropy Loss - Detailed Walkthrough\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"UNDERSTANDING CROSS-ENTROPY LOSS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Scenario: Predicting next token in vocabulary of 5 words\n",
    "# Vocabulary: [\"the\", \"cat\", \"dog\", \"runs\", \"sleeps\"]\n",
    "# True next token: \"cat\" (index 1)\n",
    "\n",
    "# Model's raw predictions (logits)\n",
    "logits = torch.tensor([[0.1, 3.0, 0.5, 0.2, 0.1]])  # Batch of 1\n",
    "target = torch.tensor([1])  # True token is at index 1\n",
    "\n",
    "print(\"\\n1. Raw Model Output (logits):\", logits.tolist())\n",
    "print(\"   True token index: 1 ('cat')\")\n",
    "\n",
    "# Method 1: Using CrossEntropyLoss (combines LogSoftmax + NLLLoss)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, target)\n",
    "print(f\"\\n2. Cross-Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Method 2: Manual calculation to understand what's happening\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "print(f\"\\n3. Probabilities after softmax:\")\n",
    "print(f\"   'the': {probabilities[0, 0].item():.4f}\")\n",
    "print(f\"   'cat': {probabilities[0, 1].item():.4f}\")\n",
    "print(f\"   'dog': {probabilities[0, 2].item():.4f}\")\n",
    "print(f\"   'runs': {probabilities[0, 3].item():.4f}\")\n",
    "print(f\"   'sleeps': {probabilities[0, 4].item():.4f}\")\n",
    "\n",
    "manual_loss = -torch.log(probabilities[0, 1])\n",
    "print(f\"\\n4. Manual Loss Calculation: -log({probabilities[0, 1].item():.4f}) = {manual_loss.item():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CODE EXAMPLE 2: Batch Training with SGD vs Adam\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZERS: SGD vs ADAM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a simple model\n",
    "class SimpleLanguageModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size=1000, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.mean(dim=1)  # Simple aggregation\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleLanguageModel()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Generate dummy data\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "vocab_size = 1000\n",
    "\n",
    "input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "targets = torch.randint(0, vocab_size, (batch_size,))\n",
    "\n",
    "print(f\"\\nTraining setup:\")\n",
    "print(f\"  Batch size: {batch_size}\")\n",
    "print(f\"  Sequence length: {seq_length}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Compare optimizers\n",
    "def train_step(optimizer, num_steps=3):\n",
    "    losses = []\n",
    "    for step in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "# Reset model\n",
    "model1 = SimpleLanguageModel()\n",
    "sgd_optimizer = torch.optim.SGD(model1.parameters(), lr=0.01)\n",
    "sgd_losses = train_step(sgd_optimizer, num_steps=5)\n",
    "\n",
    "# Reset model\n",
    "model2 = SimpleLanguageModel()\n",
    "adam_optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
    "adam_losses = train_step(adam_optimizer, num_steps=5)\n",
    "\n",
    "print(f\"\\nSGD Learning (lr=0.01):\")\n",
    "for i, loss in enumerate(sgd_losses):\n",
    "    print(f\"  Step {i+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nAdam Learning (lr=0.001):\")\n",
    "for i, loss in enumerate(adam_losses):\n",
    "    print(f\"  Step {i+1}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(f\"\\nObservations:\")\n",
    "print(f\"  SGD final loss: {sgd_losses[-1]:.4f}\")\n",
    "print(f\"  Adam final loss: {adam_losses[-1]:.4f}\")\n",
    "print(f\"  Adam converges faster! (More stable learning)\")\n",
    "\n",
    "# ============================================================================\n",
    "# CODE EXAMPLE 3: Perplexity - Understanding Model Confidence\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\\n\" + \"=\" * 60)\n",
    "print(\"PERPLEXITY: A MORE INTUITIVE METRIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "losses_to_evaluate = [0.5, 1.0, 2.0, 3.0]\n",
    "print(f\"\\nLoss → Perplexity Interpretation:\")\n",
    "print(f\"{'Loss':<8} {'Perplexity':<15} {'Interpretation'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for loss_val in losses_to_evaluate:\n",
    "    perplexity = np.exp(loss_val)\n",
    "    interpretation = f\"Model is as confused as if there were {perplexity:.1f} equally likely tokens\"\n",
    "    print(f\"{loss_val:<8} {perplexity:<15.2f} {interpretation}\")\n",
    "\n",
    "print(f\"\\nKey insight:\")\n",
    "print(f\"  - Perplexity = 1 (loss=0): Perfect predictions\")\n",
    "print(f\"  - Perplexity = 10 (loss=2.3): Model thinks top 10 tokens are equally likely\")\n",
    "print(f\"  - Perplexity = 50000 (loss=10.8): Model is very uncertain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a_fineTuning (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
